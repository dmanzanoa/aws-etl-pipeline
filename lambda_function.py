import json
import boto3
import time

"""
AWS Lambda function template for triggering an AWS Glue job and updating a Glue crawler
based on S3 events.

This example listens for S3 object creation events. When an object matching a
specific prefix and suffix is detected, the function starts a Glue ETL job,
monitors its state until completion and then updates or creates a Glue
crawler before running it. All resource identifiers are left as placeholders
so that you can customise them for your own environment without exposing
sensitive names or bucket structures.
"""

glue = boto3.client('glue')

# Replace these values with your own resource names
GLUE_JOB_NAME = "<your_glue_job_name>"
CRAWLER_NAME = "<your_crawler_name>"
CRAWLER_ROLE = "<your_crawler_role>"
CRAWLER_DATABASE = "<your_crawler_database>"
CRAWLER_TARGET_PATH = "<your_target_s3_path>"


def lambda_handler(event, context):
    """Entry point for the Lambda function.

    The function expects an event with an S3 structure as generated by
    Amazon S3 notifications. It looks for files in a specific prefix and
    suffix before starting the Glue job. Adjust the conditions in the
    `if` statement to fit your use case.

    Parameters
    ----------
    event : dict
        The event payload provided by AWS Lambda, expected to contain
        S3 notification records.
    context : LambdaContext
        Runtime information provided by AWS Lambda (unused).
    """
    for record in event.get('Records', []):
        key = record['s3']['object']['key']
        print(f"Triggered by: {key}")

        # Only trigger the Glue job for specific files
        if key.startswith("<your_prefix>/") and key.endswith(".parquet"):
            print("Valid file detected, starting Glue job...")
            response = glue.start_job_run(JobName=GLUE_JOB_NAME)
            job_run_id = response['JobRunId']
            print(f"Started Glue job {GLUE_JOB_NAME} with JobRunId: {job_run_id}")

            # Poll the job status until it completes
            while True:
                status = glue.get_job_run(JobName=GLUE_JOB_NAME, RunId=job_run_id)['JobRun']['JobRunState']
                print(f"Glue job status: {status}")
                if status in ['SUCCEEDED', 'FAILED', 'STOPPED']:
                    break
                time.sleep(30)

            if status != 'SUCCEEDED':
                raise Exception(f"Glue job did not succeed, final state: {status}")

            # Update or create the crawler
            try:
                glue.get_crawler(Name=CRAWLER_NAME)
                glue.update_crawler(
                    Name=CRAWLER_NAME,
                    Role=CRAWLER_ROLE,
                    DatabaseName=CRAWLER_DATABASE,
                    Targets={'S3Targets': [{'Path': CRAWLER_TARGET_PATH}]}
                )
                print(f"Crawler {CRAWLER_NAME} updated.")
            except glue.exceptions.EntityNotFoundException:
                glue.create_crawler(
                    Name=CRAWLER_NAME,
                    Role=CRAWLER_ROLE,
                    DatabaseName=CRAWLER_DATABASE,
                    Targets={'S3Targets': [{'Path': CRAWLER_TARGET_PATH}]},
                    SchemaChangePolicy={
                        'UpdateBehavior': 'UPDATE_IN_DATABASE',
                        'DeleteBehavior': 'DEPRECATE_IN_DATABASE'
                    }
                )
                print(f"Crawler {CRAWLER_NAME} created.")

            glue.start_crawler(Name=CRAWLER_NAME)
            print(f"Crawler {CRAWLER_NAME} started.")

            return {
                'statusCode': 200,
                'body': json.dumps('Glue job and crawler executed successfully!')
            }
        else:
            print(f"Ignoring file: {key}")

    return {
        'statusCode': 200,
        'body': json.dumps('No matching file detected; Lambda did not run.')
    }
